<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[YuLong@coder]]></title>
  <link href="http://fieryfish.github.com/atom.xml" rel="self"/>
  <link href="http://fieryfish.github.com/"/>
  <updated>2014-06-16T22:21:11+01:00</updated>
  <id>http://fieryfish.github.com/</id>
  <author>
    <name><![CDATA[Yulong]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[概率模型probabilistic models (1)]]></title>
    <link href="http://fieryfish.github.com/blog/2014/06/16/probabilistic-models/"/>
    <updated>2014-06-16T15:34:00+01:00</updated>
    <id>http://fieryfish.github.com/blog/2014/06/16/probabilistic-models</id>
    <content type="html"><![CDATA[<p>之前说到我在公司是做Ruby on rails程序员，后来转到数据这块（小公司+好老板的好处就是可以无损的转去做自己喜欢的项目），
面临的第一个项目就是做一个分类器，而我第一个采用的第一个模型就是概率模型(probabilistic models)，
具体分类器是<a href="http://zh.wikipedia.org/zh/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8">朴素贝叶斯分类器</a>。
效果在当时的我们看来，可以总结为三个字：碉堡了。我当时看到结果真的蛮震惊的，从前上课时候一扫而过的贝叶斯公式竟然有如此威力！</p>

<p>首先，如果对于朴素贝叶斯或者贝叶斯公式还不打了解的同学，我的建议是：拿出一张纸和一支笔，去<a href="http://www.ruanyifeng.com/blog/2013/12/naive_bayes_classifier.html">阮一峰的blog</a>
搜索、学习贝叶斯相关内容。（阮一峰的blog很有文采，强烈推荐之）。
在我看到‘朴素贝叶斯(Naive Bayes)’这几个字以后，对我来讲比较敏感的字眼其实是‘朴素(Naive)’两个字，
贝叶斯就贝叶斯，怎么还naive呢？
其实，他表示的是一个简化的假设（assumption），即假设数据的特征(feature)/属性/字段是条件独立的(conditional independent)，why？
举个例子（引用我的导师<a href="http://www.sussex.ac.uk/profiles/335583/publications">Novi</a>）：假设我们要训练样例是20个features组成的，每个feature都是取值二元的（即可以取0或者1，no或者yes），
样例的类别（class label）也是二元的，</p>

<script type="math/tex; mode=display">
p(X_{1},X_{2}..X_{20}|Y)
</script>

<p>那么，概率的估计（probability estimates）有多少种可能呢？也就是假设空间(hypothesis space)的大小是多少？
我的答案：
$2^{21} -1=2097151个$
（每个Feature有两种，class label有两种，减1是因为概率之和必须为1，减去一个自由度）</p>

<p>所以这就导致了我们至少要学习2097151个样例才可以保证贝叶斯公式可以用，是不是很恐怖。
这也就是为什么我们需要一个条件独立假设，即 </p>

<script type="math/tex; mode=display">
P(X_{1},X_{2}..X_{20}|Y) := P(X_{1}|Y)\cdot P(X_{2}|Y)...P(X_{20}|Y)
</script>

<p>这时候，概率的估计（probability estimates）有多少种可能呢？
我的答案是41个，
（我的解释：对于每一项P(X|Y)来说，给定了Y值，X的自由度是1，因为其概率和是1，而Y有两种可能取值，所以一共2*20=40,
再加上P(Y)还有一个自由度，一共就是41）
显然，naive Bayes小了很多，对于训练的要求减小了。
那再引出一个问题，什么是‘不朴素(non-naive)’的分类器呢？</p>

<p>回归到我当时的任务，简单场景就是：每个商铺/网店都有一些描述信息，我们想根据字段的信息（比如‘地址address’）把他们分类到相关城市city去。
做这个任务的模型很简单，首先将地址分词，得到word1,word2,word3…,得到相应公式(以分成两个词为例)：</p>

<script type="math/tex; mode=display">
P(Y_{city}|X_{word1},X_{word2})=\frac{P(X_{word1}|Y_{city})\cdot P(X_{word2}|Y_{city})\cdot (p_{Y_{city}})}{P(X_{word1})\cdot P(X_{word2})}
</script>

<p>由于分类时候，分子都是是一样的，可以不考虑。再假设
$P(Y_{city})$
是均匀分布(uniform distribution)，又可以
忽略这个值（但不总是满足uniform distribution, 先验知识有时候是很重要的），那么最终的分类过程相当于 </p>

<script type="math/tex; mode=display">
P^{\ast}(Y_{city}|X_{word1},X_{word2})=\operatorname{argmax}_Y (P(X_{word1}|Y_{city})\cdot P(X_{word2}|Y_{city}))
</script>

<p>在实际应用时候，naive Bayes有个特别的优势：有些地址是不太好判别，而这一类的地址，在计算后验概率的时候，
概率较大的几组概率都很相近，导致很难‘确信’地把这些样例分类到相关城市。比如: (长春和北京都有朝阳区哦~)</p>

<script type="math/tex; mode=display">
P(Y_{北京}|X_{朝阳区})=0.3 , ~~
P(Y_{长春}|X_{朝阳区})=0.33
</script>

<p>那么通过naive Bayes可以加一些简单的判断去提取这些很容易出错的实例，是不是很有用呢。^-^
另外,先验知识(prior knowledge)也可以被当做因素考虑进来，例如：根据上面的概率
<script type="math/tex">P(Y_{长春}|X_{朝阳区}) > P(Y_{北京}|X_{朝阳区})</script>
因此，这个实例应该被分到<strong>长春</strong>。</p>

<p>但是考虑先验知识
<script type="math/tex">P(Y_{北京})=0.5, ~~ P(Y_{长春})=0.3</script></p>

<p>那么
<script type="math/tex">% &lt;![CDATA[
P(Y_{长春}|X_{朝阳区})\cdot P(Y_{长春}) < P(Y_{北京}|X_{朝阳区})\cdot P(Y_{北京}) %]]&gt;</script>
因此，这个实例应该被分到<strong>北京</strong>。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hello Machine Learning]]></title>
    <link href="http://fieryfish.github.com/blog/2014/06/15/hello-machine-learning/"/>
    <updated>2014-06-15T18:16:00+01:00</updated>
    <id>http://fieryfish.github.com/blog/2014/06/15/hello-machine-learning</id>
    <content type="html"><![CDATA[<p>‘hello world’似乎像是技术的入场礼，不同的人总在开始的时候用尽一些办法，就是为了看到’hello XXX’这几个字母
闪现在屏幕的那一刹那。现在我也盗用这个仪式，开始聊聊我对于Machine Learning(ML)的理解。
我觉得学东西直观的感觉(intuition)和基于问题的学习很重要啊，尤其是抽象的东西，比如: ML -_-!
所以我会主要通过这样方式谈谈我理解。同样，一些拼写标点错误也请大家见谅，从小考试都是丢三落四，所以老师
都给我起了个外号”准优等”。文笔我也不会很严谨，毕竟哀家才是研究僧，说话不会像公知一样对大家负责。
该啰嗦的就啰嗦到这里了~</p>

<p>总有一些事情是你一看上去就决定要终身相伴的，这可能就是我一看到machine learning的感觉吧。
还记得一年半前，我还是一个ruby on rails程序员，也是个应届生，在一家创业公司做码农。所以对于一切都是充满好奇，却也是很傻很天真。
总爱问别人很幼稚的问题，例如：
你为啥学这个啊？你觉得我学啥对我有帮助啊？等等这些。。
直到我发现一本书，<a href="http://book.douban.com/subject/3288908/">集体智慧编程(Programming Collective Intelligence)</a>，我觉得我的下一步应该是学这样子的知识！
看起来很有用，而且也很高大上的样子。而且，恰好当时公司一个技术大牛也开始研究这个领域。我就抱大腿似的
准备跟他一起搞。后来，经过一阵小小研究后，我们竟然真的做成了一些项目，那会真是兴奋地不行不行的。后面会具体介绍。</p>

<p>其实我相信对于初学者，每个人都会觉得’我擦，这东西太难了吧！怎么需要学那么多东西。而且，它需要说得过去的数学功底。’
是的，这是事实，毋庸置疑，要不岂不是谁都可以研究它了么。但是说白了，机器学习算法(algorithm)或者模型(model)本质来说就是一个
映射(mapping)而已，你所要的不就是将你的输入(input)得到个输出(output)么，而且那些复杂的算法往往开始的起因都是很简单的，
很多复杂的地方往往都是后期人们缝缝补补、增增减减的结果。这点，吴军老师在<a href="http://book.douban.com/subject/10750155/">数学之美</a>早有论述，我也对此坚信不疑，对了，这是一本好书。
但是，我在强调一下，但是，一些基础的先验知识很重要。已<a href="http://book.douban.com/subject/1102235/">机器学习(Tom Mitchell)</a>为例，概念学习那张我至少看了4遍，
还是没懂它在讲啥，什么归纳偏置(inductive bias)，假设空间，目标函数都不知道整这些概念干嘛使。
所以嘞，我的建议就是，弄了半天还不会的就搁一边，他会再出现的，到时候你肯定会如梦方醒的，因为慢慢积累这些概念以后，
会发现他们很多都是相通的，去弄懂某个孤立点的难度肯定比掌握点和点联系、区别困难很多。</p>

<p>最后我想说说我认为的学习机器学习的技巧吧。1 会<strong>读英文</strong>。不然，就只能看<a href="http://book.douban.com/subject/1102235/">机器学习(Tom Mitchell)</a>了吧。
可能也有别的选择，但我去年去清华蹭课他们用的就是这个教材，要是不靠老师讲解而是自学，蛮难的。
有些人很容易有个误会，就是会英文这门槛太高了，其实啊，专业词语就那么多，而且读英文的难度远远小于说和写，
开始慢慢来，之后就会很舒服了。2 <strong>直观理解</strong>。很多公式用一个图就解释的明明白白的。3 <strong>结合实践</strong>。看了不一定会明白，
明白了不一定理解，理解了不一定会做，做的话可以去(Kaggle.com)练练手。4 <strong>讲给别人</strong>。
我曾经问过我导师，我什么时候才能像你一样对于ML侃侃而谈，他笑着对我说:”等你可以把他讲给别人，让别人也理解，对于
他们的问题你可以解答。”这，就是为什么我开始写这个博客吧，而且，这是捷径。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Get Start]]></title>
    <link href="http://fieryfish.github.com/blog/2013/03/17/kai-shi-bloglu-cheng/"/>
    <updated>2013-03-17T18:44:00+00:00</updated>
    <id>http://fieryfish.github.com/blog/2013/03/17/kai-shi-bloglu-cheng</id>
    <content type="html"><![CDATA[<p>一直说要搞个blog写写东西，结果拖拖拉拉一直到现在，很早之前读<a href="http://book.douban.com/subject/6709809/">暗时间</a>的时候就想搭建一个blog去总结一下知识，如今终于开始这个旅程了。我分享的东西也主要是自己的总结，如果偶尔有些话能帮助到各位看官我也就心满意足了。</p>

<p>我相信只有经过思考的知识才是有价值的，所以我的文字肯定都是自己经过思考去码的，大家如果有什么想法意见也希望思考后再传达给我。最终目标希望大家把这里视作一个平台，能让大家在这里汲取、思考。</p>

<p>为什么选择<strong>bboxers.com</strong>的域名？ 因为我喜欢节奏，尤其是bbox，我的偶像是<strong>alem</strong>。peace</p>

]]></content>
  </entry>
  
</feed>
