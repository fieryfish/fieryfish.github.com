
<!DOCTYPE HTML>
<html>
<head>
	<meta charset="utf-8">
	<title>YuLong@coder</title>
	<meta name="author" content="Yulong">

	
	<meta name="description" content="上一篇简单聊了聊维数灾难，
这节就10只小猫小狗的例子稍微延伸一下，假设现在给你100只猫猫狗狗，3维已经不能满足线性可分了，这时候
可能直到10维我们才能找到一个超平面（hyperplane）去分离这100个小猫\小狗。但这时候，会引出这样一个问题，过拟合(over-fitting)。 &hellip;">
	
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

	<link href="/atom.xml" rel="alternate" title="YuLong@coder" type="application/atom+xml">
	<link rel="canonical" href="">
	<link href="/favicon.png" rel="shortcut icon">
	<link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
	<!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
	<script src="//ajax.googleapis.com/ajax/libs/jquery/1.7.2/jquery.min.js"></script>
		<link href="/favicon.jpg" rel="shortcut icon">

</head>

<body>

<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

	<header id="header" class="inner"><h1><a href="/">YuLong@coder</a></h1>
<nav id="main-nav"><ul class="main">
	<li><a href="/">Blog</a></li>
	<li><a href="/blog/archives">Archives</a></li>
	<li><a href="/about">About me</a></li>
</ul>
</nav>
<nav id="mobile-nav">
	<div class="alignleft menu">
		<a class="button">Menu</a>
		<div class="container"><ul class="main">
	<li><a href="/">Blog</a></li>
	<li><a href="/blog/archives">Archives</a></li>
	<li><a href="/about">About me</a></li>
</ul>
</div>
	</div>
	<div class="alignright search">
		<a class="button"></a>
		<div class="container">
			<form action="http://google.com/search" method="get">
				<input type="text" name="q" results="0">
				<input type="hidden" name="q" value="site:fieryfish.github.com">
			</form>
		</div>
	</div>
</nav>
<nav id="sub-nav" class="alignright">
	<div class="social">
		
		
		
		
    
		
		
		
		
		
		<a class="rss" href="/atom.xml" title="RSS">RSS</a>
		
	</div>
	<form class="search" action="http://google.com/search" method="get">
		<input class="alignright" type="text" name="q" results="0">
		<input type="hidden" name="q" value="site:fieryfish.github.com">
	</form>
</nav>

</header>
	
		
	
	<div id="content" class="inner">


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2014/06/29/curse-learning-curve/">
		
			维数灾难到学习曲线(learning Curve)</a>
	</h2>
	<div class="entry-content">
		<p><a href="http://bboxers.com/blog/2014/06/29/curse-dim/">上一篇</a>简单聊了聊维数灾难，
这节就10只小猫小狗的例子稍微延伸一下，假设现在给你100只猫猫狗狗，3维已经不能满足线性可分了，这时候
可能直到10维我们才能找到一个超平面（hyperplane）去分离这100个小猫\小狗。但这时候，会引出这样一个问题，过拟合(over-fitting)。
也就是说，这时候尽管10维feature分离100个实例效果很棒，但是对于其他未知的小猫小狗(比如第101,102..个)，它的效果可能并不好！
也就是说，训练数据(training data)的分类效果好并不等同于分类器在测试/未知数据(testing data)的效果好！
我们所追求的应该是更少的泛化误差（generalization error）。</p>

<p>泛化的反面，就是<strong>记录</strong>（memorize）。10维的超平面在这里更像是<strong>记录</strong>我这100个动物是小猫或是小狗，
而不是<strong>学习</strong>(learning)。举个不恰当的例子：
还是之前的小猫\小狗，3维数据可以很好区分10个instances，假设到第11个小猫/小狗，3个feature可能不够用了，那么就增加到4个！
然而这时你这第11个小猫小狗如果是noisy的，即误分类的，就也被‘记录’了下来，而每次发生feature不够用的情况，咱就增加一个feature，
最后就相当于你用了10个feature记录下了100个小动物是小猫还是小狗而已，即使这里面有误分类的，也给无情的记录下来了。
这种记录，对于其他待分类数据，是无用的。
这也就是为什么Decision Tree要选择‘最短路径’，Concept Learning要选择合取式(Conjunctive normal form)而不是析取。
<!--这就是为什么我们有了一系列像cross-validation的方法。--></p>

<p><img class="left" src="/images/learning_overfitting.png" width="400" height="400" title="image" alt="Using too many features results in overfitting" /></p>

<p>(这里还要先请大家请自行google 一下cross-validation和bias/variance trade-off，因为后面的内容都是基于cross-validation的，
bias/variance的概念也是要涉及到的。)</p>

<p>对于之前那样的过拟合问题，显然是一个high variance问题，这类问题我们的学习曲线(Learning Curve)大概是这样的：
<img class="left" src="/images/learning1.png" width="400" height="400" title="image" alt="From Novi 课件" /></p>

<p>相反地，一个high bias问题的Learning Curve：</p>

<p><img class="left" src="/images/learning2.png" width="400" height="400" title="image" alt="From Novi 课件" /></p>

<p>对于分类问题，我们并不知道可以达到的最优结果(desired performance)是多少，
不过Learning Curve却是一个可以让你往这个方向去的直观的工具。
通过画Learning Curve，我们至少可以对现有的分类器有种‘感觉’，加之corss-validation，进而去避免high variance/high bias，
进而再通过优化自己的参数free parameter，去接近、达到desired performance。</p>

		
		
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2014-06-29T15:47:00+01:00" pubdate data-updated="true">Jun 29<span>th</span>, 2014</time></div>
	<div class="tags">

</div>
	
</div></article>


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2014/06/29/curse-dim/">
		
			聊聊维数灾难(Curse of Dimensionality)</a>
	</h2>
	<div class="entry-content">
		<p>本文原文摘自<a href="http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/">这里</a>,作部分修改。</p>

<p>这篇文章，聊聊很普遍的一个机器学习现象Curse of Dimensionality。首先，维度怎么会是一种‘灾难’呢？通过下面这个
非常直观的例子，我们来看看。
现在有这么一个分类任务，就是要区分10只‘狗’or‘猫’，给定的特征(feature)是基于颜色的（红、绿、蓝）。现在呢，我们考虑第一个
feature——<strong>红色</strong>。</p>

<p><img class="left" src="/images/curse1.png" width="350" height="350" title="image" alt="1维" /></p>

<p>感觉不是太好？不能线性可分？那就再考虑一种feature——<strong>绿色</strong>！现在我们有2维feature了哦。</p>

<p><img class="left" src="/images/curse2.png" width="350" height="350" title="image" alt="2维" /></p>

<p>感觉还是不是太好？还是不能线性可分？那就再再考虑一种feature——<strong>蓝色</strong>！现在我们有3维feature了哦~</p>

<p><img class="left" src="/images/curse3.png" width="350" height="350" title="image" alt="3维" /></p>

<p>哇塞，现在可以找到一个平面去分离小猫小狗了！</p>

<p><img class="left" src="/images/curse3yes.png" width="350" height="350" title="image" alt="3维--线性可分" /></p>

<p>这里要告诉大家一个事实，<strong>当维度越来越高的时候，那么线性可分的概率会越来越大，而且当其维度高到一定时候，线性可分的概率会到1</strong>。(源自Cover theorem)。
插播一下，我上面说的这个事实也是kernel trick的基础，所以本文跟kernel method有很大关系，他们的目的很简单，就是处理线性不可分数据，而处理的方法本质一样，而细节略有不同。
比如，来说RBF network可以使用Guassian function作为RBF function去处理线性不可分数据，SVM可能会选择kernel trick直接在高维度处理数据等等。
kernel method是机器学习很重要的一块，并不是说一定只能使用在SVM。我经常会觉得ML有时候更像是拼积木的游戏，
你想搭建一个城堡（SVM处理线性不可分数据），那你就会看看哪个积木（kernel trick）更适用，然后就拿过来用，
而积木(kernel trick)并不是说从属于你这所城堡。</p>

<p>回到直播，那么这时候我们会说：越高维度越好喽！
很不幸，维度高的‘灾难’这是显现出来了。就我们这个例子来说，假设每个维度有5个单位间隔（unit intervals），
开个玩笑的说：不太红，有点红，红，特别红，极度红。-_-! 那么10个instances的情况下，每个单位间隔有2个instance喽。
同样的问题升到2维，还是10个instances，但我们这时有5 x 5 = 25个单位间隔（unit squares）。
平均每个单位间隔这时只有 10/25 = 0.4个instance。 再升到3维就只有0.08了。
那这里的变化量就是我们的密度(Density), 也就是密度变得越来越小了，越来越稀疏(sparse)了。</p>

<p>下面这个图从另一个角度解释了维度高的‘灾难’。如果，分类器的feature的范围是0到1的连续情况，1维时候，
涵盖20%的feature只需要从全体（population）选20%个训练数据就可以了。但是如果升到2维，
我们确需要从全体选出45%的数据才能满足涵盖20%feature(0.45x0.45=0.2)。3维去要0.58(0.58^3=0.2)。
看来，相同的涵盖率，在维度增长时候，我们需要的训练数据是指数增加的。细细想想，也是很直观的，
更多的feature当然需要更多的训练数据喽，我在<a href="http://bboxers.com/blog/2014/06/16/probabilistic-models/">概率模型probabilistic Models (1)</a>
也聊过关于训练数据多少的问题。</p>

<p><img class="left" src="/images/curse4a.png" width="600" height="600" title="image" alt="1-&gt;3维" /></p>

<p>下面一节我还会继续这个话题，先休息一下~不过是另外一个角度，主要是说learning curve。</p>

		
		
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2014-06-29T14:17:00+01:00" pubdate data-updated="true">Jun 29<span>th</span>, 2014</time></div>
	<div class="tags">

</div>
	
</div></article>


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2014/06/24/models/">
		
			俯瞰Machine Learning</a>
	</h2>
	<div class="entry-content">
		<p>这篇的内容主要是摘自<a href="http://book.douban.com/subject/11453073/">Machine Learning</a>
这本书第一章节，我觉得蛮有收获的，希望可以提供一个不一样的角度。
另外，这本书真的不错，是我导师推荐的，最重要的是我们Sussex大学图书馆竟然有！我会告诉大家我们图书馆连一本介绍Ruby的书籍都没有么。。可能是因为这本书是Cambridge出的，作为英国同乡总要给些面子吧，就引进来了。</p>

<p>既然是‘俯瞰’，那肯定不是某些特定的技术。首先，问大家这样一个问题”Given a real world problem, how would you
reduce it to a ml problem.(一个实际问题如何转化成一个ML问题？)”
<a href="http://www.reddit.com/r/MachineLearning/comments/1wmayh/ml_job_interview_questions/">引自reddit</a>。
这样的问题好像很没有头绪，但是别慌。书里给出了一个蛮不错的答案：其实大多数机器学习问题都着重干的3件事是关于Task, Model, Feature的。
具体来说，实际的问题到底是要处理哪个Task：分类(classfication)? 回归(regression)? 聚类(clustering)?
而不同的问题也要根据其场景去选择不同的model（稍后详细描述）。
在真正使用模型的时候，更重要的可能是feature的选取、生成。例如：在SVM中的kernel trick就是一种关于feature的操作，亦或者
pre-processing。通常来讲，我们大多数情况下不能直接获取到适用于model的数据，自然要对feature进行一些处理喽。</p>

<p>这里我重点想聊一聊Model。在作者看来，Model有三大类型：Logical model, Probabilistic model, Geometry model.
大家可以试着回顾一下3种model里面包含的算法。这里我分别举例一下他们典型的代表: Decision Tree, Naive Bayes, SVM。
这样分类有什么好处呢就是各个类别有其特点。比如 Logical model比较直观(主要是Tree的表现形式)，其很多思想源自computer science
(如<a href="http://en.wikipedia.org/wiki/Disjunctive_normal_form">DNF</a>,<a href="http://en.wikipedia.org/wiki/Conjunctive_normal_form">CNF</a>)。
Probabilistic model的思想则更多是源自数学喽(尤其是概率与统计)。Geometry model也是很直观的，几乎所有课件都会来个二维平面，
然后各种正例反例加个decision boundary去解释来解释去，当然也免不了最优化，解析几何的一些数学概念。</p>

<p>在处理实际问题时，不知道大家有没有在意过这样一个角度，这些模型其实还有个区别就是在于输入数据的类型，即<strong>离散值</strong>、<strong>连续值</strong>。
其对应的概念就是Grouping model, Grading model。举个例子：训练数据里面有两个正例为(5.0, 3.0), (1.3, 2.4)，两个个反例为(-5.0, -3.0), (-1.3, -2.4)。
对于Geometry model可能直接画个直线(decision boundary)，然后告诉你在直线的一侧，任何数据都是正例，另一侧都是反例(连续的感觉有木有)。
而对于Decision Tree,可能会问你这样的问题，对于新来的数据(x,y), x和y &gt; 0乎?(离散的感觉有木有) 要是大于0呢，就是正例，否则呢，就都是反例。
从<a href="http://book.douban.com/subject/11453073/">书里</a>摘两个图给大家，希望大家能有个俯瞰的感觉。</p>

<p>字段说明(geometry model, probabilistic model, logical model, grouping model, grading model, discrete value, real value, supervised, unsupervised, multi-class)</p>

<p><img class="left" src="/images/model1.png" width="600" height="600" title="image" alt="总结" />
<img class="left" src="/images/model2.png" width="600" height="600" title="image" alt="总结2" /></p>

		
		
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2014-06-24T12:01:00+01:00" pubdate data-updated="true">Jun 24<span>th</span>, 2014</time></div>
	<div class="tags">

</div>
	
</div></article>


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2014/06/18/perceptron/">
		
			感知器(Perceptron)的边边角角</a>
	</h2>
	<div class="entry-content">
		<p>感知器是相当重要的，原因很简单，它是Network的基础，不论是ANN，SVM，他们都是基于感知器的。
这也是为什么我在学校修Neural Network这门课时，光Perceptron的作业就占了50%的分数。
算法很简单，看看图很容易理解。所以我在主要想谈谈这里面一些其他蛮有意思的东西。</p>

<p>首先，想问下感知器的损失函数（loss function）的形式什么？
在很多情况下，我们用到的损失函数形式都是这个样子的：(引用自<a href="http://www.sussex.ac.uk/sussexneuroscience/people/person/201607">Luc</a>课件)</p>

<p><script type="math/tex">E(\bar{w})=\frac{1}{2}\Sigma(t_{i}-y_{i})^2</script> 
(1)</p>

<p>其中，y是感知器对于ith实例(instance)的输出(+1,-1)，t是ith实例的类别(+1,-1)。</p>

<p>为什么对于线性可分数据，普遍的感知器的loss function的选择却变成了这个样子？</p>

<p><script type="math/tex">E(\bar{w})=-\Sigma \bar{w} \cdot \bar{x_{i}} \cdot t_{i} </script>
（for all misclassified $\bar{x_{i}}$） (2)</p>

<p>其中w是权重，x是ith实例(instance)的值，t是ith实例的类别。</p>

<p><img class="left" src="/images/perceptron.png" width="350" height="350" title="image" alt="perceptron" /></p>

<p>由这个图，我们可以看到，(2)式可以告诉我们更多误分类(misclassified)点的信息，
具体来说，就是误分类点距离分界线(decision boundary)的远近正比于损失函数，这样损失函数越大，说明误分类点离得越远，反之亦然。
而(1)式对于所有误分类点一概而论，显然没有(2)来的合理。</p>

<p>至于更新法则：</p>

<p><script type="math/tex">\bar{w}(t+1) = \bar{w}(t) + \eta \bar{x_{i}} \cdot t_{i} </script>
（for all misclassified $\bar{x_{i}}$）</p>

<p>(<a href="http://book.douban.com/subject/10590856/">统计学习方法</a>给出的形式)</p>

<p>等价于:</p>

<script type="math/tex; mode=display">\bar{w}(t+1) = \bar{w}(t) + \eta \bar{x_{i}} \cdot (t_{i}-y_{i}) </script>

<p>$y_{i}$感知器对于ith实例的输出。</p>

<p>(<a href="http://book.douban.com/subject/1102235/">机器学习(Tom Mitchell)</a>给出的形式)</p>

<p>另外的一个问题，对于我们这个问题，我们应该用批量(batch)梯度下降还是随机(stochastic)梯度下降呢？</p>

<p>我的答案是随机(stochastic)梯度下降吧。(<a href="http://www.quora.com/Machine-Learning/Whats-the-difference-between-gradient-descent-and-stochastic-gradient-descent">参考</a>)
不过前提是数据线性可分，不然会导致不能收敛。</p>

<p>这一次我们做了几个选择题，而大家也可以看到，Perceptron的模型(model)很简单，重点是要在适当的时刻选择一些适当的工具（如：loss function形式, batch/stochastic），
这里，不得不说，ML是一个选择是世界，一个算法的好坏，往往不在于算法本身，而在于根据数据进行的选择，
比如SVM，如果都是用默认参数，起初的效果一般都不是很理想，但是可以慢慢调节参数以后，大都会提高很明显。说了这么多，就是强调一下：<strong>选择</strong>很重要。</p>

		
		
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2014-06-18T19:17:00+01:00" pubdate data-updated="true">Jun 18<span>th</span>, 2014</time></div>
	<div class="tags">

</div>
	
</div></article>


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2014/06/16/probabilistic-models/">
		
			概率模型probabilistic Models (1)</a>
	</h2>
	<div class="entry-content">
		<p>之前说到我在公司是做Ruby on rails程序员，后来转到数据这块（小公司+好老板的好处就是可以无损的转去做自己喜欢的项目），
面临的第一个项目就是做一个分类器，而我第一个采用的第一个模型就是概率模型(probabilistic models)，
具体分类器是<a href="http://zh.wikipedia.org/zh/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8">朴素贝叶斯分类器</a>。
效果在当时的我们看来，可以总结为三个字：碉堡了。我当时看到结果真的蛮震惊的，从前上课时候一扫而过的贝叶斯公式竟然有如此威力！</p>

<p>首先，如果对于朴素贝叶斯或者贝叶斯公式还不打了解的同学，我的建议是：拿出一张纸和一支笔，去<a href="http://www.ruanyifeng.com/blog/2013/12/naive_bayes_classifier.html">阮一峰的blog</a>
搜索、学习贝叶斯相关内容。（阮一峰的blog很有文采，强烈推荐之）。
在我看到‘朴素贝叶斯(Naive Bayes)’这几个字以后，对我来讲比较敏感的字眼其实是‘朴素(Naive)’两个字，
贝叶斯就贝叶斯，怎么还naive呢？
其实，他表示的是一个简化的假设（assumption），即假设数据的特征(feature)/属性/字段是条件独立的(conditional independent)，why？
举个例子（引用我的导师<a href="http://www.sussex.ac.uk/profiles/335583/publications">Novi</a>）：假设我们要训练样例是20个features组成的，每个feature都是取值二元的（即可以取0或者1，no或者yes），
样例的类别（class label）也是二元的，</p>

<script type="math/tex; mode=display">
p(X_{1},X_{2}..X_{20}|Y)
</script>

<p>那么，概率的估计（probability estimates）有多少种可能呢？也就是假设空间(hypothesis space)的大小是多少？
我的答案：
$2^{21} -1=2097151个$
（每个Feature有两种，class label有两种，减1是因为概率之和必须为1，减去一个自由度）</p>

<p>所以这就导致了我们至少要学习2097151个样例才可以保证贝叶斯公式可以用，是不是很恐怖。
这也就是为什么我们需要一个条件独立假设，即 </p>

<script type="math/tex; mode=display">
P(X_{1},X_{2}..X_{20}|Y) := P(X_{1}|Y)\cdot P(X_{2}|Y)...P(X_{20}|Y)
</script>

<p>这时候，概率的估计（probability estimates）有多少种可能呢？
我的答案是41个，
（我的解释：对于每一项P(X|Y)来说，给定了Y值，X的自由度是1，因为其概率和是1，而Y有两种可能取值，所以一共2*20=40,
再加上P(Y)还有一个自由度，一共就是41）
显然，naive Bayes小了很多，对于训练的要求减小了。
那再引出一个问题，什么是‘不朴素(non-naive)’的分类器呢？</p>

<p>回归到我当时的任务，简单场景就是：每个商铺/网店都有一些描述信息，我们想根据字段的信息（比如‘地址address’）把他们分类到相关城市city去。
做这个任务的模型很简单，首先将地址分词，得到word1,word2,word3…,得到相应公式(以分成两个词为例)：</p>

<script type="math/tex; mode=display">
P(Y_{city}|X_{word1},X_{word2})=\frac{P(X_{word1}|Y_{city})\cdot P(X_{word2}|Y_{city})\cdot (p_{Y_{city}})}{P(X_{word1})\cdot P(X_{word2})}
</script>

<p>由于分类时候，分子都是是一样的，可以不考虑。再假设
$P(Y_{city})$
是均匀分布(uniform distribution)，又可以
忽略这个值（但不总是满足uniform distribution, 先验知识有时候是很重要的），那么最终的分类过程相当于 </p>

<script type="math/tex; mode=display">
P^{\ast}(Y_{city}|X_{word1},X_{word2})=\operatorname{argmax}_Y (P(X_{word1}|Y_{city})\cdot P(X_{word2}|Y_{city}))
</script>

<p>在实际应用时候，naive Bayes有个特别的优势：有些地址是不太好判别，而这一类的地址，在计算后验概率的时候，
概率较大的几组概率都很相近，导致很难‘确信’地把这些样例分类到相关城市。比如: (长春和北京都有朝阳区哦~)</p>

<script type="math/tex; mode=display">
P(Y_{北京}|X_{朝阳区})=0.3 , ~~
P(Y_{长春}|X_{朝阳区})=0.33
</script>

<p>那么通过naive Bayes可以加一些简单的判断去提取这些很容易出错的实例，是不是很有用呢。^-^
另外,先验知识(prior knowledge)也可以被当做因素考虑进来，例如：根据上面的概率
<script type="math/tex">P(Y_{长春}|X_{朝阳区}) > P(Y_{北京}|X_{朝阳区})</script>
因此，这个实例应该被分到<strong>长春</strong>。</p>

<p>但是考虑先验知识
<script type="math/tex">P(Y_{北京})=0.5, ~~ P(Y_{长春})=0.3</script></p>

<p>那么
<script type="math/tex">% <![CDATA[
P(Y_{长春}|X_{朝阳区})\cdot P(Y_{长春}) < P(Y_{北京}|X_{朝阳区})\cdot P(Y_{北京}) %]]></script>
因此，这个实例应该被分到<strong>北京</strong>。</p>


		
		
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2014-06-16T15:34:00+01:00" pubdate data-updated="true">Jun 16<span>th</span>, 2014</time></div>
	<div class="tags">

</div>
	
</div></article>


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2014/06/15/hello-machine-learning/">
		
			Hello Machine Learning</a>
	</h2>
	<div class="entry-content">
		<p>‘hello world’似乎像是技术的入场礼，不同的人总在开始的时候用尽一些办法，就是为了看到’hello XXX’这几个字母
闪现在屏幕的那一刹那。现在我也盗用这个仪式，开始聊聊我对于Machine Learning(ML)的理解。
我觉得学东西直观的感觉(intuition)和基于问题的学习很重要啊，尤其是抽象的东西，比如: ML -_-!
所以我会主要通过这样方式谈谈我理解。同样，一些拼写标点错误也请大家见谅，从小考试都是丢三落四，所以老师
都给我起了个外号”准优等”。文笔我也不会很严谨，毕竟哀家才是研究僧，说话不会像公知一样对大家负责。
该啰嗦的就啰嗦到这里了~</p>

<p>总有一些事情是你一看上去就决定要终身相伴的，这可能就是我一看到machine learning的感觉吧。
还记得一年半前，我还是一个ruby on rails程序员，也是个应届生，在一家创业公司做码农。所以对于一切都是充满好奇，却也是很傻很天真。
总爱问别人很幼稚的问题，例如：
你为啥学这个啊？你觉得我学啥对我有帮助啊？等等这些。。
直到我发现一本书，<a href="http://book.douban.com/subject/3288908/">集体智慧编程(Programming Collective Intelligence)</a>，我觉得我的下一步应该是学这样子的知识！
看起来很有用，而且也很高大上的样子。而且，恰好当时公司一个技术大牛也开始研究这个领域。我就抱大腿似的
准备跟他一起搞。后来，经过一阵小小研究后，我们竟然真的做成了一些项目，那会真是兴奋地不行不行的。后面会具体介绍。</p>

<p>其实我相信对于初学者，每个人都会觉得’我擦，这东西太难了吧！怎么需要学那么多东西。而且，它需要说得过去的数学功底。’
是的，这是事实，毋庸置疑，要不岂不是谁都可以研究它了么。但是说白了，机器学习算法(algorithm)或者模型(model)本质来说就是一个
映射(mapping)而已，你所要的不就是将你的输入(input)得到个输出(output)么，而且那些复杂的算法往往开始的起因都是很简单的，
很多复杂的地方往往都是后期人们缝缝补补、增增减减的结果。这点，吴军老师在<a href="http://book.douban.com/subject/10750155/">数学之美</a>早有论述，我也对此坚信不疑，对了，这是一本好书。
但是，我在强调一下，但是，一些基础的先验知识很重要。已<a href="http://book.douban.com/subject/1102235/">机器学习(Tom Mitchell)</a>为例，概念学习那张我至少看了4遍，
还是没懂它在讲啥，什么归纳偏置(inductive bias)，假设空间，目标函数都不知道整这些概念干嘛使。
所以嘞，我的建议就是，弄了半天还不会的就搁一边，他会再出现的，到时候你肯定会如梦方醒的，因为慢慢积累这些概念以后，
会发现他们很多都是相通的，去弄懂某个孤立点的难度肯定比掌握点和点联系、区别困难很多。</p>

<p>最后我想说说我认为的学习机器学习的技巧吧。1 会<strong>读英文</strong>。不然，就只能看<a href="http://book.douban.com/subject/1102235/">机器学习(Tom Mitchell)</a>了吧。
可能也有别的选择，但我去年去清华蹭课他们用的就是这个教材，要是不靠老师讲解而是自学，蛮难的。
有些人很容易有个误会，就是会英文这门槛太高了，其实啊，专业词语就那么多，而且读英文的难度远远小于说和写，
开始慢慢来，之后就会很舒服了。2 <strong>直观理解</strong>。很多公式用一个图就解释的明明白白的。3 <strong>结合实践</strong>。看了不一定会明白，
明白了不一定理解，理解了不一定会做，做的话可以去(Kaggle.com)练练手。4 <strong>讲给别人</strong>。
我曾经问过我导师，我什么时候才能像你一样对于ML侃侃而谈，他笑着对我说:”等你可以把他讲给别人，让别人也理解，对于
他们的问题你可以解答。”这，就是为什么我开始写这个博客吧，而且，这是捷径。
5 <strong>基础知识</strong>,尽管数学面积很大，但至少梯度下降，贝叶斯这些必备知识要完全掌握。</p>


		
		
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2014-06-15T18:16:00+01:00" pubdate data-updated="true">Jun 15<span>th</span>, 2014</time></div>
	<div class="tags">

</div>
	
</div></article>


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2013/03/17/kai-shi-bloglu-cheng/">
		
			Get Start</a>
	</h2>
	<div class="entry-content">
		<p>一直说要搞个blog写写东西，结果拖拖拉拉一直到现在，很早之前读<a href="http://book.douban.com/subject/6709809/">暗时间</a>的时候就想搭建一个blog去总结一下知识，如今终于开始这个旅程了。我分享的东西也主要是自己的总结，如果偶尔有些话能帮助到各位看官我也就心满意足了。</p>

<p>我相信只有经过思考的知识才是有价值的，所以我的文字肯定都是自己经过思考去码的，大家如果有什么想法意见也希望思考后再传达给我。最终目标希望大家把这里视作一个平台，能让大家在这里汲取、思考。</p>

<p>为什么选择<strong>bboxers.com</strong>的域名？ 因为我喜欢节奏，尤其是bbox，我的偶像是<strong>alem</strong>。peace</p>


		
		
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2013-03-17T18:44:00+00:00" pubdate data-updated="true">Mar 17<span>th</span>, 2013</time></div>
	<div class="tags">

</div>
	
</div></article>

<nav id="pagenavi">
    
    
    <div class="center"><a href="/blog/archives">Blog Archives</a></div>
</nav></div>
	<footer id="footer" class="inner">Copyright &copy; 2014

    Yulong

</footer>
	<script src="/javascripts/slash.js"></script>
<script src="/javascripts/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
	$('.fancybox').fancybox();
})(jQuery);
</script> <!-- Delete or comment this line to disable Fancybox -->





<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-44691622-3', 'bboxers.com');
  ga('send', 'pageview');

</script>
    <!--<script type="text/javascript">-->
        <!--var _gaq = _gaq || [];-->
        <!--_gaq.push(['_setAccount', 'UA-44691622-3']);-->
        <!--_gaq.push(['_setDomainName','bboxers.com']);-->
        <!--_gaq.push(['_trackPageview']);-->

        <!--(function() {-->
            <!--var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;-->
            <!--ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';-->
            <!--var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);-->
        <!--})();-->
    <!--</script>-->



</body>
</html>
