
<!DOCTYPE HTML>
<html>
<head>
	<meta charset="utf-8">
	<title>YuLong@coder</title>
	<meta name="author" content="Yulong">

	
	<meta name="description" content="首先BIC是干什么的？用于比较在给定的数据下，比较不同模型的准则(criterion)。
怎么比较呢？他会关注两件事情，一更好的拟合结果，二更少的参数。一很好理解，二这是要求
一个相对更简单的模型去防止overfit这种事情。在想不起来的，请看这幅图。
既然是比较模型，那它也属于model &hellip;">
	
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

	<link href="/atom.xml" rel="alternate" title="YuLong@coder" type="application/atom+xml">
	<link rel="canonical" href="">
	<link href="/favicon.png" rel="shortcut icon">
	<link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
	<!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
	<script src="//ajax.googleapis.com/ajax/libs/jquery/1.7.2/jquery.min.js"></script>
		<link href="/favicon.jpg" rel="shortcut icon">

</head>

<body>

<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

	<header id="header" class="inner"><h1><a href="/">YuLong@coder</a></h1>
<nav id="main-nav"><ul class="main">
	<li><a href="/">Blog</a></li>
	<li><a href="/blog/archives">Archives</a></li>
	<li><a href="/about">About me</a></li>
</ul>
</nav>
<nav id="mobile-nav">
	<div class="alignleft menu">
		<a class="button">Menu</a>
		<div class="container"><ul class="main">
	<li><a href="/">Blog</a></li>
	<li><a href="/blog/archives">Archives</a></li>
	<li><a href="/about">About me</a></li>
</ul>
</div>
	</div>
	<div class="alignright search">
		<a class="button"></a>
		<div class="container">
			<form action="http://google.com/search" method="get">
				<input type="text" name="q" results="0">
				<input type="hidden" name="q" value="site:fieryfish.github.com">
			</form>
		</div>
	</div>
</nav>
<nav id="sub-nav" class="alignright">
	<div class="social">
		
		
		
		
    
		
		
		
		
		
		<a class="rss" href="/atom.xml" title="RSS">RSS</a>
		
	</div>
	<form class="search" action="http://google.com/search" method="get">
		<input class="alignright" type="text" name="q" results="0">
		<input type="hidden" name="q" value="site:fieryfish.github.com">
	</form>
</nav>

</header>
	
		
	
	<div id="content" class="inner">


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2014/08/18/bic/">
		
			BIC(bayesian Information Criterion)</a>
	</h2>
	<div class="entry-content">
		<p>首先BIC是干什么的？用于比较在给定的数据下，比较不同模型的准则(criterion)。
怎么比较呢？他会关注两件事情，一更好的拟合结果，二更少的参数。一很好理解，二这是要求
一个相对更简单的模型去防止overfit这种事情。在想不起来的，请看这幅<a href="http://zoonek2.free.fr/UNIX/48_R/g883.png">图</a>。
既然是比较模型，那它也属于model selection的范畴了。
在<a href="http://en.wikipedia.org/wiki/Bayesian_information_criterion">wiki</a>上写了好多感觉还蛮啰嗦的，
开始看了几遍也不怎么知道啥意思，其实主要的公式就是:(实例instance很多的情况下)</p>

<script type="math/tex; mode=display">BIC = -2\cdot ln \hat{L}+k\cdot ln(n)</script>

<p>$\hat{L}$是已经完成了像MLE这种参数估计以后，得到的最优参数的模型。k是参数的个数，n是实例的个数。
一例胜千言，请大家直接翻到这个很棒的<a href="http://hea-www.cfa.harvard.edu/astrostat/Stat310_0910/dr_20100323_mle.pdf">ppt</a>，29页
有一个简单的例子，相信看完这个例子的人基本也就明白是怎么回事了。</p>

		
		
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2014-08-18T23:14:00+01:00" pubdate data-updated="true">Aug 18<span>th</span>, 2014</time></div>
	<div class="tags">

</div>
	
		<div class="comments"><a href="/blog/2014/08/18/bic//index.html#disqus_thread">Comments</a></div>
	
</div></article>


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2014/07/17/ggm/">
		
			高斯图模型 Gaussian Graphical Model</a>
	</h2>
	<div class="entry-content">
		<p>如果说以前的一些文章还有加入自己的思考的话，那这篇可能是最客观的表述。其实
Gaussian graphical model(GGM)资料还真的蛮少的，我本来也想找找书籍去学习下，结果到最后还是
要去看论文。当然，论文的力量就是可以把一个知识讲到令人发指的长，这篇blog主要是梳理GGM的主要脉络，
有兴趣的同学请直接参考相关论文文献。(Yuan and Lin, 2007 ; Friedman et al. 2008)</p>

<p>首先，什么是Gaussian graphical model？它围绕Gaussian分布的变量去研究各个变量的联系。具体来说就是
要研究Multivariate Normal Distribution.（插一句，这里Normal就是Gaussian，同义词，指正态分布，一般
我们会把很多变量假设为正态分布的，比较典型的就是回归问题里error项拉,其原因就是中心极限定理central limit theorem。）
Multivariate Normal Distribution的核心，是inverse covariance matrix(ICM)，也叫concentraion matrix, precision matrix.
ICM在Multivariate Normal Distribution的密度函数里面，他决定了Gaussian graphical model长什么样子哦。
好了，晕了的同学不好意思，必须要自己wiki一下Multivariate Normal Distribution，
尤其是一个重要的性质：设</p>

<p>covariance matrix: <script type="math/tex">\Sigma</script> ,</p>

<p>inverse covariance matrix:C = $(\Sigma)^{-1} $</p>

<p>$C=(c_{ij})$</p>

<p>如果<script type="math/tex">c_{ij}=0</script>，那么变量i和变量j是独立的。也就是说，高斯图里，两个变量是没有联系的。
学过图论的都知道，图就是node和edge组成的，如果$c_{ij}=0$，则那两个nodes没有edge。
简单屡一下，高斯图模型重点是确定ICM长成什么样子，因为里面的元素为0代表相应的两个变量独立，也就确立了图的样子。
下面我们来聊一聊另一个我们追求的，就是’稀疏’（sparse）。什么意思呢？在图里，不是每一个连接（Edge）都是我们想要的，
换句话说，有时候nodes之间的edge有可能是噪音，应该被去除的，
而又有时候，你只想看哪些nodes之间的连接是显著相关的（significant）。所以这里有2个问题：其一，去除更多
不是很相关的连接，也就是让ICM里面有更多的0，即ICM更加sparse。方法？Lasso regularization。
其二，这是一个trade-off，
如果你想要保留更多的edges，lasso的惩罚项相应的就要小一点,保留的信息也就更多，
反之，你增大lasso的惩罚项，edges就少了，图也就更sparse了一些。</p>

<p>基于上述讨论，我直接给出我们要求解的表达式，具体过程请参考：(Yuan and Lin, 2007)</p>

<p>最大化：$log det (C) − tr(SC) − ρ||C||_{1}$</p>

<p>S is empirical covariance matrix.</p>

<p>解决上述最优化方程有很多方法，像Interior point methods等等，我在我的毕业设计采用了<a href="http://statweb.stanford.edu/~tibs/glasso/">glsso</a>(Friedman et al. 2008)。
它的优点就是快！他采用了block coordinate descent算法去求解上式，这个算法感觉就是奔着快去的。在这个<a href="https://www.cs.cmu.edu/~ggordon/10725-F12/schedule.html">链接</a>里面的
Lecture 25有它的详细介绍，如果论文看不明白可以参考这个教程，其实我现在也在慢慢啃这个知识点，感觉还蛮难的，最优化没学好。。呜呜。。</p>

<p>Anyway,我的毕业设计是关于犬类疾病研究的，最后自己画了一匹藏獒和相应的gaussian graphical model.<a href="http://smileclinic.alwaysdata.net/jackmsc/ggm_dog_network/">Link</a>.</p>


		
		
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2014-07-17T10:18:00+01:00" pubdate data-updated="true">Jul 17<span>th</span>, 2014</time></div>
	<div class="tags">

</div>
	
		<div class="comments"><a href="/blog/2014/07/17/ggm//index.html#disqus_thread">Comments</a></div>
	
</div></article>


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2014/07/11/logister/">
		
			概率模型probabilistic Model(2) Logistic回归</a>
	</h2>
	<div class="entry-content">
		<p>故事是这样的，在概率模型世界，我们要面对的问题就是这样的一个公式：
<script type="math/tex">p(y|\bar{x})</script>,
其中，y是类别(class), $\bar{x}$是输入数据(input data)。
在<a href="http://bboxers.com/blog/2014/06/16/probabilistic-models/">Naive Bayes</a>里面我们已经看到了如何用Bayes公式搞定这个问题。
这次，我们可以用个更直观的方式，怎么搞？
这样？
<script type="math/tex">p(y|\bar{x})=w_{0}+\Sigma(w_{i} \cdot x_{i})</script>,
左边很熟悉（probabilistic model），右边也很熟悉(linear model)，但是，放在一起总是怪怪的。
计算机界有个名言，”Any problem in computer science can 
be solved by adding a layer of indirection”
这次，我们的’layer’就是logistic function。
没听过的同学请先看<a href="http://en.wikipedia.org/wiki/Logistic_function">wiki</a>的解释。
记住公式、记住图的样子，别问为啥。好了，再记住一个它的性质，1-$\theta(x) = \theta(-x)(\theta$表示logistic function)。对比图像看看，怎么样？很直观的一个性质吧。
logistic function就是把右边那个linear model的范围(无穷)缩小到probabilistic model的范围([0,1])了，从而可以让我们用概率的方法搞定它。</p>

<p>既然回归到了概率问题，那就用概率的方式解决。考虑一个普遍的binary问题：</p>

<p><img class="left" src="/images/binary.png" width="400" height="400" title="image" alt="Binary" /></p>

<p>它同样是<a href="http://en.wikipedia.org/wiki/Bernoulli_distribution">bernoulli distribution</a>。</p>

<p>引入logistic function, 
<script type="math/tex">\theta(\bar{w}\cdot \bar{x})= \theta(w_{0}+\Sigma(w_{i} \cdot x_{i}))=\frac{1}{1+exp(-(w_{0}+\Sigma(w_{i} \cdot x_{i})))}</script></p>

<p>把它带入上面的方程，得到了:
<script type="math/tex">P(y=+1|\bar{w}\cdot \bar{x})= \frac{1}{1+exp(-(w_{0}+\Sigma(w_{i} \cdot x_{i})))}</script></p>

<p>和
<script type="math/tex">P(y=-1|\bar{w}\cdot \bar{x})=1-\frac{1}{1+exp(-(w_{0}+\Sigma(w_{i} \cdot x_{i})))}</script></p>

<p>然后：
<script type="math/tex">\frac{P(y=+1|\bar{w}\cdot \bar{x})}{P(y=-1|\bar{w}\cdot \bar{x})}= exp(w_{0}+\bar{w}\cdot \bar{x})</script></p>

<p>两边取对数：
<script type="math/tex">ln\frac{P(y=+1|\bar{w}\cdot \bar{x})}{P(y=-1|\bar{w}\cdot \bar{x})}=w_{0}+\bar{w}\cdot \bar{x}</script></p>

<p>很神奇吧？分类器出现了！这种神奇当然要归功于logistic function和它的性质喽！</p>

<p>然后呢，就是训练喽。这里用到的就是概率里面常用到的Maximum likelihood方法。一些推导我直接copy了，请注意
最后的那个就是目标函数。</p>

<p><img class="left" src="/images/logis.png" width="600" height="400" title="image" alt="推导" /></p>

<p>问题又被转化为了最优化问题，它是convex的，有最优解，用Gradient Descent就完事了~这里不是课本，就是说这个思路拉。
再补一句，logistic regression会有过拟合问题，需要regularization。
经典方式又出现了: argmin(目标函数+regularization)</p>

<p>logistic regression在我看来是一个有很多标签的分类器。1 人们经常用它作为discriminative model的代表，
跟generative model的代表naive Bayes去比较(<a href="http://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf">比如这篇</a>)。
discriminative model &amp; generative model请大家好好注意一下，这个
区别存在于很多其他地方哦。
(1)generative model是基于p(x,y), discriminative model是基于p(y|x),这样看来discriminative model更加直接，
对于直接的分类问题，准确性相对更高，而且p(x,y)会储存更多值
(2)generative model和discriminative可以互相转化，p(y|x)*p(x)就是p(x,y)。具体问题具体分析，有时候你可能要构建(x,y)而不是(x|y)。
2 它的名字虽然包含regression，但是它做的事情是classification。
3 它是个线性分类器。</p>


		
		
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2014-07-11T00:18:00+01:00" pubdate data-updated="true">Jul 11<span>th</span>, 2014</time></div>
	<div class="tags">

</div>
	
		<div class="comments"><a href="/blog/2014/07/11/logister//index.html#disqus_thread">Comments</a></div>
	
</div></article>


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2014/06/29/curse-learning-curve/">
		
			维数灾难到学习曲线(learning Curve)</a>
	</h2>
	<div class="entry-content">
		<p><a href="http://bboxers.com/blog/2014/06/29/curse-dim/">上一篇</a>简单聊了聊维数灾难，
这节就10只小猫小狗的例子稍微延伸一下，假设现在给你100只猫猫狗狗，3维已经不能满足线性可分了，这时候
可能直到10维我们才能找到一个超平面（hyperplane）去分离这100个小猫\小狗。但这时候，会引出这样一个问题，过拟合(over-fitting)。
也就是说，这时候尽管10维feature分离100个实例效果很棒，但是对于其他未知的小猫小狗(比如第101,102..个)，它的效果可能并不好！
也就是说，训练数据(training data)的分类效果好并不等同于分类器在测试/未知数据(testing data)的效果好！
我们所追求的应该是更少的泛化误差（generalization error）。</p>

<p>泛化的反面，就是<strong>记录</strong>（memorize）。10维的超平面在这里更像是<strong>记录</strong>我这100个动物是小猫或是小狗，
而不是<strong>学习</strong>(learning)。举个不恰当的例子：
还是之前的小猫\小狗，3维数据可以很好区分10个instances，假设到第11个小猫/小狗，3个feature可能不够用了，那么就增加到4个！
然而这时你这第11个小猫小狗如果是noisy的，即误分类的，就也被‘记录’了下来，而每次发生feature不够用的情况，咱就增加一个feature，
最后就相当于你用了10个feature记录下了100个小动物是小猫还是小狗而已，即使这里面有误分类的，也给无情的记录下来了。
这种记录，对于其他待分类数据，是无用的。
这也就是为什么Decision Tree要选择‘最短路径’，Concept Learning要选择合取式(Conjunctive normal form)而不是析取。
<!--这就是为什么我们有了一系列像cross-validation的方法。--></p>

<p><img class="left" src="/images/learning_overfitting.png" width="400" height="400" title="image" alt="Using too many features results in overfitting" /></p>

<p>(这里还要先请大家请自行google 一下cross-validation和bias/variance trade-off，因为后面的内容都是基于cross-validation的，
bias/variance的概念也是要涉及到的。)</p>

<p>对于之前那样的过拟合问题，显然是一个high variance问题，这类问题我们的学习曲线(Learning Curve)大概是这样的：
<img class="left" src="/images/learning1.png" width="400" height="400" title="image" alt="From Novi 课件" /></p>

<p>相反地，一个high bias问题的Learning Curve：</p>

<p><img class="left" src="/images/learning2.png" width="400" height="400" title="image" alt="From Novi 课件" /></p>

<p>对于分类问题，我们并不知道可以达到的最优结果(desired performance)是多少，
不过Learning Curve却是一个可以让你往这个方向去的直观的工具。
通过画Learning Curve，我们至少可以对现有的分类器有种‘感觉’，加之corss-validation，进而去避免high variance/high bias，
进而再通过优化自己的参数free parameter，去接近、达到desired performance。</p>

		
		
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2014-06-29T15:47:00+01:00" pubdate data-updated="true">Jun 29<span>th</span>, 2014</time></div>
	<div class="tags">

</div>
	
		<div class="comments"><a href="/blog/2014/06/29/curse-learning-curve//index.html#disqus_thread">Comments</a></div>
	
</div></article>


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2014/06/29/curse-dim/">
		
			聊聊维数灾难(Curse of Dimensionality)</a>
	</h2>
	<div class="entry-content">
		<p>本文原文摘自<a href="http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/">这里</a>,作部分修改。</p>

<p>这篇文章，聊聊很普遍的一个机器学习现象Curse of Dimensionality。首先，维度怎么会是一种‘灾难’呢？通过下面这个
非常直观的例子，我们来看看。
现在有这么一个分类任务，就是要区分10只‘狗’or‘猫’，给定的特征(feature)是基于颜色的（红、绿、蓝）。现在呢，我们考虑第一个
feature——<strong>红色</strong>。</p>

<p><img class="left" src="/images/curse1.png" width="350" height="350" title="image" alt="1维" /></p>

<p>感觉不是太好？不能线性可分？那就再考虑一种feature——<strong>绿色</strong>！现在我们有2维feature了哦。</p>

<p><img class="left" src="/images/curse2.png" width="350" height="350" title="image" alt="2维" /></p>

<p>感觉还是不是太好？还是不能线性可分？那就再再考虑一种feature——<strong>蓝色</strong>！现在我们有3维feature了哦~</p>

<p><img class="left" src="/images/curse3.png" width="350" height="350" title="image" alt="3维" /></p>

<p>哇塞，现在可以找到一个平面去分离小猫小狗了！</p>

<p><img class="left" src="/images/curse3yes.png" width="350" height="350" title="image" alt="3维--线性可分" /></p>

<p>这里要告诉大家一个事实，<strong>当维度越来越高的时候，那么线性可分的概率会越来越大，而且当其维度高到一定时候，线性可分的概率会到1</strong>。(源自Cover theorem)。
插播一下，我上面说的这个事实也是kernel trick的基础，所以本文跟kernel method有很大关系，他们的目的很简单，就是处理线性不可分数据，而处理的方法本质一样，而细节略有不同。
比如，来说RBF network可以使用Guassian function作为RBF function去处理线性不可分数据，SVM可能会选择kernel trick直接在高维度处理数据等等。
kernel method是机器学习很重要的一块，并不是说一定只能使用在SVM。我经常会觉得ML有时候更像是拼积木的游戏，
你想搭建一个城堡（SVM处理线性不可分数据），那你就会看看哪个积木（kernel trick）更适用，然后就拿过来用，
而积木(kernel trick)并不是说从属于你这所城堡。</p>

<p>回到直播，那么这时候我们会说：越高维度越好喽！
很不幸，维度高的‘灾难’这是显现出来了。就我们这个例子来说，假设每个维度有5个单位间隔（unit intervals），
开个玩笑的说：不太红，有点红，红，特别红，极度红。-_-! 那么10个instances的情况下，每个单位间隔有2个instance喽。
同样的问题升到2维，还是10个instances，但我们这时有5 x 5 = 25个单位间隔（unit squares）。
平均每个单位间隔这时只有 10/25 = 0.4个instance。 再升到3维就只有0.08了。
那这里的变化量就是我们的密度(Density), 也就是密度变得越来越小了，越来越稀疏(sparse)了。</p>

<p>下面这个图从另一个角度解释了维度高的‘灾难’。如果，分类器的feature的范围是0到1的连续情况，1维时候，
涵盖20%的feature只需要从全体（population）选20%个训练数据就可以了。但是如果升到2维，
我们确需要从全体选出45%的数据才能满足涵盖20%feature(0.45x0.45=0.2)。3维去要0.58(0.58^3=0.2)。
看来，相同的涵盖率，在维度增长时候，我们需要的训练数据是指数增加的。细细想想，也是很直观的，
更多的feature当然需要更多的训练数据喽，我在<a href="http://bboxers.com/blog/2014/06/16/probabilistic-models/">概率模型probabilistic Models (1)</a>
也聊过关于训练数据多少的问题。</p>

<p><img class="left" src="/images/curse4a.png" width="600" height="600" title="image" alt="1-&gt;3维" /></p>

<p>下面一节我还会继续这个话题，先休息一下~不过是另外一个角度，主要是说learning curve。</p>

		
		
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2014-06-29T14:17:00+01:00" pubdate data-updated="true">Jun 29<span>th</span>, 2014</time></div>
	<div class="tags">

</div>
	
		<div class="comments"><a href="/blog/2014/06/29/curse-dim//index.html#disqus_thread">Comments</a></div>
	
</div></article>


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2014/06/24/models/">
		
			俯瞰Machine Learning</a>
	</h2>
	<div class="entry-content">
		<p>这篇的内容主要是摘自<a href="http://book.douban.com/subject/11453073/">Machine Learning</a>
这本书第一章节，我觉得蛮有收获的，希望可以提供一个不一样的角度。
另外，这本书真的不错，是我导师推荐的，最重要的是我们Sussex大学图书馆竟然有！我会告诉大家我们图书馆连一本介绍Ruby的书籍都没有么。。可能是因为这本书是Cambridge出的，作为英国同乡总要给些面子吧，就引进来了。</p>

<p>既然是‘俯瞰’，那肯定不是某些特定的技术。首先，问大家这样一个问题”Given a real world problem, how would you
reduce it to a ml problem.(一个实际问题如何转化成一个ML问题？)”
<a href="http://www.reddit.com/r/MachineLearning/comments/1wmayh/ml_job_interview_questions/">引自reddit</a>。
这样的问题好像很没有头绪，但是别慌。书里给出了一个蛮不错的答案：其实大多数机器学习问题都着重干的3件事是关于Task, Model, Feature的。
具体来说，实际的问题到底是要处理哪个Task：分类(classfication)? 回归(regression)? 聚类(clustering)?
而不同的问题也要根据其场景去选择不同的model（稍后详细描述）。
在真正使用模型的时候，更重要的可能是feature的选取、生成。例如：在SVM中的kernel trick就是一种关于feature的操作，亦或者
pre-processing。通常来讲，我们大多数情况下不能直接获取到适用于model的数据，自然要对feature进行一些处理喽。</p>

<p>这里我重点想聊一聊Model。在作者看来，Model有三大类型：Logical model, Probabilistic model, Geometry model.
大家可以试着回顾一下3种model里面包含的算法。这里我分别举例一下他们典型的代表: Decision Tree, Naive Bayes, SVM。
这样分类有什么好处呢就是各个类别有其特点。比如 Logical model比较直观(主要是Tree的表现形式)，其很多思想源自computer science
(如<a href="http://en.wikipedia.org/wiki/Disjunctive_normal_form">DNF</a>,<a href="http://en.wikipedia.org/wiki/Conjunctive_normal_form">CNF</a>)。
Probabilistic model的思想则更多是源自数学喽(尤其是概率与统计)。Geometry model也是很直观的，几乎所有课件都会来个二维平面，
然后各种正例反例加个decision boundary去解释来解释去，当然也免不了最优化，解析几何的一些数学概念。</p>

<p>在处理实际问题时，不知道大家有没有在意过这样一个角度，这些模型其实还有个区别就是在于输入数据的类型，即<strong>离散值</strong>、<strong>连续值</strong>。
其对应的概念就是Grouping model, Grading model。举个例子：训练数据里面有两个正例为(5.0, 3.0), (1.3, 2.4)，两个个反例为(-5.0, -3.0), (-1.3, -2.4)。
对于Geometry model可能直接画个直线(decision boundary)，然后告诉你在直线的一侧，任何数据都是正例，另一侧都是反例(连续的感觉有木有)。
而对于Decision Tree,可能会问你这样的问题，对于新来的数据(x,y), x和y &gt; 0乎?(离散的感觉有木有) 要是大于0呢，就是正例，否则呢，就都是反例。
从<a href="http://book.douban.com/subject/11453073/">书里</a>摘两个图给大家，希望大家能有个俯瞰的感觉。</p>

<p>字段说明(geometry model, probabilistic model, logical model, grouping model, grading model, discrete value, real value, supervised, unsupervised, multi-class)</p>

<p><img class="left" src="/images/model1.png" width="600" height="600" title="image" alt="总结" />
<img class="left" src="/images/model2.png" width="600" height="600" title="image" alt="总结2" /></p>

		
		
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2014-06-24T12:01:00+01:00" pubdate data-updated="true">Jun 24<span>th</span>, 2014</time></div>
	<div class="tags">

</div>
	
		<div class="comments"><a href="/blog/2014/06/24/models//index.html#disqus_thread">Comments</a></div>
	
</div></article>


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2014/06/18/perceptron/">
		
			感知器(Perceptron)的边边角角</a>
	</h2>
	<div class="entry-content">
		<p>感知器是相当重要的，原因很简单，它是Network的基础，不论是ANN，SVM，他们都是基于感知器的。
这也是为什么我在学校修Neural Network这门课时，光Perceptron的作业就占了50%的分数。
算法很简单，看看图很容易理解。所以我在主要想谈谈这里面一些其他蛮有意思的东西。</p>

<p>首先，想问下感知器的损失函数（loss function）的形式什么？
在很多情况下，我们用到的损失函数形式都是这个样子的：(引用自<a href="http://www.sussex.ac.uk/sussexneuroscience/people/person/201607">Luc</a>课件)</p>

<p><script type="math/tex">E(\bar{w})=\frac{1}{2}\Sigma(t_{i}-y_{i})^2</script> 
(1)</p>

<p>其中，y是感知器对于ith实例(instance)的输出(+1,-1)，t是ith实例的类别(+1,-1)。</p>

<p>为什么对于线性可分数据，普遍的感知器的loss function的选择却变成了这个样子？</p>

<p><script type="math/tex">E(\bar{w})=-\Sigma \bar{w} \cdot \bar{x_{i}} \cdot t_{i} </script>
（for all misclassified $\bar{x_{i}}$） (2)</p>

<p>其中w是权重，x是ith实例(instance)的值，t是ith实例的类别。</p>

<p><img class="left" src="/images/perceptron.png" width="350" height="350" title="image" alt="perceptron" /></p>

<p>由这个图，我们可以看到，(2)式可以告诉我们更多误分类(misclassified)点的信息，
具体来说，就是误分类点距离分界线(decision boundary)的远近正比于损失函数，这样损失函数越大，说明误分类点离得越远，反之亦然。
而(1)式对于所有误分类点一概而论，显然没有(2)来的合理。</p>

<p>至于更新法则：</p>

<p><script type="math/tex">\bar{w}(t+1) = \bar{w}(t) + \eta \bar{x_{i}} \cdot t_{i} </script>
（for all misclassified $\bar{x_{i}}$）</p>

<p>(<a href="http://book.douban.com/subject/10590856/">统计学习方法</a>给出的形式)</p>

<p>等价于:</p>

<script type="math/tex; mode=display">\bar{w}(t+1) = \bar{w}(t) + \eta \bar{x_{i}} \cdot (t_{i}-y_{i}) </script>

<p>$y_{i}$感知器对于ith实例的输出。</p>

<p>(<a href="http://book.douban.com/subject/1102235/">机器学习(Tom Mitchell)</a>给出的形式)</p>

<p>另外的一个问题，对于我们这个问题，我们应该用批量(batch)梯度下降还是随机(stochastic)梯度下降呢？</p>

<p>我的答案是随机(stochastic)梯度下降吧。(<a href="http://www.quora.com/Machine-Learning/Whats-the-difference-between-gradient-descent-and-stochastic-gradient-descent">参考</a>)
不过前提是数据线性可分，不然会导致不能收敛。</p>

<p>这一次我们做了几个选择题，而大家也可以看到，Perceptron的模型(model)很简单，重点是要在适当的时刻选择一些适当的工具（如：loss function形式, batch/stochastic），
这里，不得不说，ML是一个选择是世界，一个算法的好坏，往往不在于算法本身，而在于根据数据进行的选择，
比如SVM，如果都是用默认参数，起初的效果一般都不是很理想，但是可以慢慢调节参数以后，大都会提高很明显。说了这么多，就是强调一下：<strong>选择</strong>很重要。</p>

		
		
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2014-06-18T19:17:00+01:00" pubdate data-updated="true">Jun 18<span>th</span>, 2014</time></div>
	<div class="tags">

</div>
	
		<div class="comments"><a href="/blog/2014/06/18/perceptron//index.html#disqus_thread">Comments</a></div>
	
</div></article>


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2014/06/16/probabilistic-models/">
		
			概率模型probabilistic Models (1)</a>
	</h2>
	<div class="entry-content">
		<p>之前说到我在公司是做Ruby on rails程序员，后来转到数据这块（小公司+好老板的好处就是可以无损的转去做自己喜欢的项目），
面临的第一个项目就是做一个分类器，而我第一个采用的第一个模型就是概率模型(probabilistic models)，
具体分类器是<a href="http://zh.wikipedia.org/zh/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8">朴素贝叶斯分类器</a>。
效果在当时的我们看来，可以总结为三个字：碉堡了。我当时看到结果真的蛮震惊的，从前上课时候一扫而过的贝叶斯公式竟然有如此威力！</p>

<p>首先，如果对于朴素贝叶斯或者贝叶斯公式还不打了解的同学，我的建议是：拿出一张纸和一支笔，去<a href="http://www.ruanyifeng.com/blog/2013/12/naive_bayes_classifier.html">阮一峰的blog</a>
搜索、学习贝叶斯相关内容。（阮一峰的blog很有文采，强烈推荐之）。
在我看到‘朴素贝叶斯(Naive Bayes)’这几个字以后，对我来讲比较敏感的字眼其实是‘朴素(Naive)’两个字，
贝叶斯就贝叶斯，怎么还naive呢？
其实，他表示的是一个简化的假设（assumption），即假设数据的特征(feature)/属性/字段是条件独立的(conditional independent)，why？
举个例子（引用我的导师<a href="http://www.sussex.ac.uk/profiles/335583/publications">Novi</a>）：假设我们要训练样例是20个features组成的，每个feature都是取值二元的（即可以取0或者1，no或者yes），
样例的类别（class label）也是二元的，</p>

<script type="math/tex; mode=display">
p(X_{1},X_{2}..X_{20}|Y)
</script>

<p>那么，概率的估计（probability estimates）有多少种可能呢？也就是假设空间(hypothesis space)的大小是多少？
我的答案：
$2^{21} -1=2097151个$
（每个Feature有两种，class label有两种，减1是因为概率之和必须为1，减去一个自由度）</p>

<p>所以这就导致了我们至少要学习2097151个样例才可以保证贝叶斯公式可以用，是不是很恐怖。
这也就是为什么我们需要一个条件独立假设，即 </p>

<script type="math/tex; mode=display">
P(X_{1},X_{2}..X_{20}|Y) := P(X_{1}|Y)\cdot P(X_{2}|Y)...P(X_{20}|Y)
</script>

<p>这时候，概率的估计（probability estimates）有多少种可能呢？
我的答案是41个，
（我的解释：对于每一项P(X|Y)来说，给定了Y值，X的自由度是1，因为其概率和是1，而Y有两种可能取值，所以一共2*20=40,
再加上P(Y)还有一个自由度，一共就是41）
显然，naive Bayes小了很多，对于训练的要求减小了。
那再引出一个问题，什么是‘不朴素(non-naive)’的分类器呢？</p>

<p>回归到我当时的任务，简单场景就是：每个商铺/网店都有一些描述信息，我们想根据字段的信息（比如‘地址address’）把他们分类到相关城市city去。
做这个任务的模型很简单，首先将地址分词，得到word1,word2,word3…,得到相应公式(以分成两个词为例)：</p>

<script type="math/tex; mode=display">
P(Y_{city}|X_{word1},X_{word2})=\frac{P(X_{word1}|Y_{city})\cdot P(X_{word2}|Y_{city})\cdot (p_{Y_{city}})}{P(X_{word1})\cdot P(X_{word2})}
</script>

<p>由于分类时候，分子都是是一样的，可以不考虑。再假设
$P(Y_{city})$
是均匀分布(uniform distribution)，又可以
忽略这个值（但不总是满足uniform distribution, 先验知识有时候是很重要的），那么最终的分类过程相当于 </p>

<script type="math/tex; mode=display">
P^{\ast}(Y_{city}|X_{word1},X_{word2})=\operatorname{argmax}_Y (P(X_{word1}|Y_{city})\cdot P(X_{word2}|Y_{city}))
</script>

<p>在实际应用时候，naive Bayes有个特别的优势：有些地址是不太好判别，而这一类的地址，在计算后验概率的时候，
概率较大的几组概率都很相近，导致很难‘确信’地把这些样例分类到相关城市。比如: (长春和北京都有朝阳区哦~)</p>

<script type="math/tex; mode=display">
P(Y_{北京}|X_{朝阳区})=0.3 , ~~
P(Y_{长春}|X_{朝阳区})=0.33
</script>

<p>那么通过naive Bayes可以加一些简单的判断去提取这些很容易出错的实例，是不是很有用呢。^-^
另外,先验知识(prior knowledge)也可以被当做因素考虑进来，例如：根据上面的概率
<script type="math/tex">P(Y_{长春}|X_{朝阳区}) > P(Y_{北京}|X_{朝阳区})</script>
因此，这个实例应该被分到<strong>长春</strong>。</p>

<p>但是考虑先验知识
<script type="math/tex">P(Y_{北京})=0.5, ~~ P(Y_{长春})=0.3</script></p>

<p>那么
<script type="math/tex">% <![CDATA[
P(Y_{长春}|X_{朝阳区})\cdot P(Y_{长春}) < P(Y_{北京}|X_{朝阳区})\cdot P(Y_{北京}) %]]></script>
因此，这个实例应该被分到<strong>北京</strong>。</p>


		
		
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2014-06-16T15:34:00+01:00" pubdate data-updated="true">Jun 16<span>th</span>, 2014</time></div>
	<div class="tags">

</div>
	
		<div class="comments"><a href="/blog/2014/06/16/probabilistic-models//index.html#disqus_thread">Comments</a></div>
	
</div></article>


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2014/06/15/hello-machine-learning/">
		
			Hello Machine Learning</a>
	</h2>
	<div class="entry-content">
		<p>‘hello world’似乎像是技术的入场礼，不同的人总在开始的时候用尽一些办法，就是为了看到’hello XXX’这几个字母
闪现在屏幕的那一刹那。现在我也盗用这个仪式，开始聊聊我对于Machine Learning(ML)的理解。
我觉得学东西直观的感觉(intuition)和基于问题的学习很重要啊，尤其是抽象的东西，比如: ML -_-!
所以我会主要通过这样方式谈谈我理解。同样，一些拼写标点错误也请大家见谅，从小考试都是丢三落四，所以老师
都给我起了个外号”准优等”。文笔我也不会很严谨，毕竟哀家才是研究僧，说话不会像公知一样对大家负责。
该啰嗦的就啰嗦到这里了~</p>

<p>总有一些事情是你一看上去就决定要终身相伴的，这可能就是我一看到machine learning的感觉吧。
还记得一年半前，我还是一个ruby on rails程序员，也是个应届生，在一家创业公司做码农。所以对于一切都是充满好奇，却也是很傻很天真。
总爱问别人很幼稚的问题，例如：
你为啥学这个啊？你觉得我学啥对我有帮助啊？等等这些。。
直到我发现一本书，<a href="http://book.douban.com/subject/3288908/">集体智慧编程(Programming Collective Intelligence)</a>，我觉得我的下一步应该是学这样子的知识！
看起来很有用，而且也很高大上的样子。而且，恰好当时公司一个技术大牛也开始研究这个领域。我就抱大腿似的
准备跟他一起搞。后来，经过一阵小小研究后，我们竟然真的做成了一些项目，那会真是兴奋地不行不行的。后面会具体介绍。</p>

<p>其实我相信对于初学者，每个人都会觉得’我擦，这东西太难了吧！怎么需要学那么多东西。而且，它需要说得过去的数学功底。’
是的，这是事实，毋庸置疑，要不岂不是谁都可以研究它了么。但是说白了，机器学习算法(algorithm)或者模型(model)本质来说就是一个
映射(mapping)而已，你所要的不就是将你的输入(input)得到个输出(output)么，而且那些复杂的算法往往开始的起因都是很简单的，
很多复杂的地方往往都是后期人们缝缝补补、增增减减的结果。这点，吴军老师在<a href="http://book.douban.com/subject/10750155/">数学之美</a>早有论述，我也对此坚信不疑，对了，这是一本好书。
但是，我在强调一下，但是，一些基础的先验知识很重要。已<a href="http://book.douban.com/subject/1102235/">机器学习(Tom Mitchell)</a>为例，概念学习那张我至少看了4遍，
还是没懂它在讲啥，什么归纳偏置(inductive bias)，假设空间，目标函数都不知道整这些概念干嘛使。
所以嘞，我的建议就是，弄了半天还不会的就搁一边，他会再出现的，到时候你肯定会如梦方醒的，因为慢慢积累这些概念以后，
会发现他们很多都是相通的，去弄懂某个孤立点的难度肯定比掌握点和点联系、区别困难很多。</p>

<p>最后我想说说我认为的学习机器学习的技巧吧。1 会<strong>读英文</strong>。不然，就只能看<a href="http://book.douban.com/subject/1102235/">机器学习(Tom Mitchell)</a>了吧。
可能也有别的选择，但我去年去清华蹭课他们用的就是这个教材，要是不靠老师讲解而是自学，蛮难的。
有些人很容易有个误会，就是会英文这门槛太高了，其实啊，专业词语就那么多，而且读英文的难度远远小于说和写，
开始慢慢来，之后就会很舒服了。2 <strong>直观理解</strong>。很多公式用一个图就解释的明明白白的。3 <strong>结合实践</strong>。看了不一定会明白，
明白了不一定理解，理解了不一定会做，做的话可以去(Kaggle.com)练练手。4 <strong>讲给别人</strong>。
我曾经问过我导师，我什么时候才能像你一样对于ML侃侃而谈，他笑着对我说:”等你可以把他讲给别人，让别人也理解，对于
他们的问题你可以解答。”这，就是为什么我开始写这个博客吧，而且，这是捷径。
5 <strong>基础知识</strong>,尽管数学面积很大，但至少梯度下降，贝叶斯这些必备知识要完全掌握。</p>


		
		
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2014-06-15T18:16:00+01:00" pubdate data-updated="true">Jun 15<span>th</span>, 2014</time></div>
	<div class="tags">

</div>
	
		<div class="comments"><a href="/blog/2014/06/15/hello-machine-learning//index.html#disqus_thread">Comments</a></div>
	
</div></article>


    <article class="post">
	<h2 class="title">
		
		<a href="/blog/2013/03/17/kai-shi-bloglu-cheng/">
		
			Get Start</a>
	</h2>
	<div class="entry-content">
		<p>一直说要搞个blog写写东西，结果拖拖拉拉一直到现在，很早之前读<a href="http://book.douban.com/subject/6709809/">暗时间</a>的时候就想搭建一个blog去总结一下知识，如今终于开始这个旅程了。我分享的东西也主要是自己的总结，如果偶尔有些话能帮助到各位看官我也就心满意足了。</p>

<p>我相信只有经过思考的知识才是有价值的，所以我的文字肯定都是自己经过思考去码的，大家如果有什么想法意见也希望思考后再传达给我。最终目标希望大家把这里视作一个平台，能让大家在这里汲取、思考。</p>

<p>为什么选择<strong>bboxers.com</strong>的域名？ 因为我喜欢节奏，尤其是bbox，我的偶像是<strong>alem</strong>。peace</p>


		
		
	</div>


<div class="meta">
	<div class="date">








  


<time datetime="2013-03-17T18:44:00+00:00" pubdate data-updated="true">Mar 17<span>th</span>, 2013</time></div>
	<div class="tags">

</div>
	
		<div class="comments"><a href="/blog/2013/03/17/kai-shi-bloglu-cheng//index.html#disqus_thread">Comments</a></div>
	
</div></article>

<nav id="pagenavi">
    
    
    <div class="center"><a href="/blog/archives">Blog Archives</a></div>
</nav></div>
	<footer id="footer" class="inner">Copyright &copy; 2014

    Yulong

</footer>
	<script src="/javascripts/slash.js"></script>
<script src="/javascripts/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
	$('.fancybox').fancybox();
})(jQuery);
</script> <!-- Delete or comment this line to disable Fancybox -->


<script type="text/javascript">
      var disqus_shortname = 'bboxerscom';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-44691622-3', 'bboxers.com');
  ga('send', 'pageview');

</script>
    <!--<script type="text/javascript">-->
        <!--var _gaq = _gaq || [];-->
        <!--_gaq.push(['_setAccount', 'UA-44691622-3']);-->
        <!--_gaq.push(['_setDomainName','bboxers.com']);-->
        <!--_gaq.push(['_trackPageview']);-->

        <!--(function() {-->
            <!--var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;-->
            <!--ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';-->
            <!--var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);-->
        <!--})();-->
    <!--</script>-->



</body>
</html>
